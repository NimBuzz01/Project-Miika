{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"# Importing libraries\nimport re\n\n# Reading text files into variables\nlines = open('../input/dataset/lines.txt', encoding='utf-8',\n             errors='ignore').read().split('\\n')\n\nconversation = open('../input/dataset/conversations.txt', encoding='utf-8',\n                    errors='ignore').read().split('\\n')\n\n# Extracting the required data from dataset\nconvoFlow = []\nfor convo in conversation:\n    convoFlow.append(convo.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \" \").replace(\",\", \"\").split())\n\ndialogs = {}\nfor line in lines:\n    dialogs[line.split(' +++$+++ ')[0]] = line.split(' +++$+++ ')[-1]\n\n## delete\ndel (lines, conversation)\n\n# Mapping Answers to Questions\nquestions = []\nanswers = []\n\n# Convo Replies(Answers) get mapped to Convos(Questions)\nfor convo in convoFlow:\n    for i in range(len(convo) - 1):\n        questions.append(dialogs[convo[i]])\n        answers.append(dialogs[convo[i + 1]])\n\n## delete\ndel (dialogs, convoFlow)\n\n#        max_len = 13         #\n# Sorting dialogs based on length.\nsortedQues = []\nsortedAns = []\nfor i in range(len(questions)):\n    if len(questions[i]) < 13:\n        sortedQues.append(questions[i])\n        sortedAns.append(answers[i])\n\n\n# Formatting all punctuations\ndef formatText(txt):\n    txt = txt.lower()\n    txt = re.sub(r\"i'm\", \"i am\", txt)\n    txt = re.sub(r\"he's\", \"he is\", txt)\n    txt = re.sub(r\"she's\", \"she is\", txt)\n    txt = re.sub(r\"that's\", \"that is\", txt)\n    txt = re.sub(r\"what's\", \"what is\", txt)\n    txt = re.sub(r\"where's\", \"where is\", txt)\n    txt = re.sub(r\"\\'ll\", \" will\", txt)\n    txt = re.sub(r\"\\'ve\", \" have\", txt)\n    txt = re.sub(r\"\\'re\", \" are\", txt)\n    txt = re.sub(r\"\\'d\", \" would\", txt)\n    txt = re.sub(r\"won't\", \"will not\", txt)\n    txt = re.sub(r\"can't\", \"can not\", txt)\n    txt = re.sub(r\"[^\\w\\s]\", \"\", txt)\n    return txt\n\n\n# Appending the formatted text\nformattedQues = []\nformattedAns = []\n\nfor line in sortedQues:\n    formattedQues.append(formatText(line))\n\nfor line in sortedAns:\n    formattedAns.append(formatText(line))\n\n## delete\ndel (answers, questions)\n\nfor i in range(len(formattedAns)):\n    formattedAns[i] = ' '.join(formattedAns[i].split()[:11])\n\ndel (sortedAns, sortedQues)\n\n# Limiting dataset to 30000 values\nformattedAns = formattedAns[:30000]\nformattedQues = formattedQues[:30000]\n## delete\n\n\n# Removing similar sentences\nword2count = {}\n\nfor line in formattedQues:\n    for word in line.split():\n        if word not in word2count:\n            word2count[word] = 1\n        else:\n            word2count[word] += 1\nfor line in formattedAns:\n    for word in line.split():\n        if word not in word2count:\n            word2count[word] = 1\n        else:\n            word2count[word] += 1\n\n# Removing words with less than 5 word count\nthreshold = 5\n\nvocab = {}\nword_num = 0\nfor word, count in word2count.items():\n    if count >= threshold:\n        vocab[word] = word_num\n        word_num += 1\n\n## delete\ndel (word2count, threshold)\ndel word_num\n\n# Adding tokens to classify text\nfor i in range(len(formattedAns)):\n    formattedAns[i] = '<SOS> ' + formattedAns[i] + ' <EOS>'\n\ntokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\nx = len(vocab)\nfor token in tokens:\n    vocab[token] = x\n    x += 1\n\n# Replacing dataset names with variables\nvocab['cameron'] = vocab['<PAD>']\nvocab['<PAD>'] = 0\n\n## delete\ndel tokens\ndel x\n\n# Inversing Vocabulary\ninv_vocab = {w: v for v, w in vocab.items()}\n\n## delete\ndel i\n\n# Input for passing text to Calculate Output\nencoderInput = []\nfor line in formattedQues:\n    tempList = []\n    for word in line.split():\n        if word not in vocab:\n            tempList.append(vocab['<OUT>'])\n        else:\n            tempList.append(vocab[word])\n\n    encoderInput.append(tempList)\n\ndecoderInput = []\nfor line in formattedAns:\n    tempList = []\n    for word in line.split():\n        if word not in vocab:\n            tempList.append(vocab['<OUT>'])\n        else:\n            tempList.append(vocab[word])\n    decoderInput.append(tempList)\n\n### delete\ndel (formattedAns, formattedQues)\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nencoderInput = pad_sequences(encoderInput, 13, padding='post', truncating='post')\ndecoderInput = pad_sequences(decoderInput, 13, padding='post', truncating='post')\n\ndecoder_final_output = []\nfor i in decoderInput:\n    decoder_final_output.append(i[1:])\n\ndecoder_final_output = pad_sequences(decoder_final_output, 13, padding='post', truncating='post')\n\ndel i\n\nfrom tensorflow.keras.utils import to_categorical\n\ndecoder_final_output = to_categorical(decoder_final_output, len(vocab))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Machine Learning Model**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Input\n\nenc_inp = Input(shape=(13,))\ndec_inp = Input(shape=(13,))\n\nVOCAB_SIZE = len(vocab)\nembed = Embedding(VOCAB_SIZE + 1, output_dim=50,\n                  input_length=13,\n                  trainable=True\n                  )\n\nenc_embed = embed(enc_inp)\nenc_lstm = LSTM(400, return_sequences=True, return_state=True)\nenc_op, h, c = enc_lstm(enc_embed)\nenc_states = [h, c]\n\ndec_embed = embed(dec_inp)\ndec_lstm = LSTM(400, return_sequences=True, return_state=True)\ndec_op, _, _ = dec_lstm(dec_embed, initial_state=enc_states)\n\ndense = Dense(VOCAB_SIZE, activation='softmax')\n\ndense_op = dense(dec_op)\n\nmodel = Model([enc_inp, dec_inp], dense_op)\n\nmodel.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n\nmodel.fit([encoderInput, decoderInput], decoder_final_output, epochs=40)\n\nmodel.save('miika_model.h5')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inference**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom keras.models import load_model\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\n\nmodel = load_model('miika_model.h5')\nenc_model = Model([enc_inp], enc_states)\n\n# decoder Model\ndecoder_state_input_h = Input(shape=(400,))\ndecoder_state_input_c = Input(shape=(400,))\n\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\ndecoder_outputs, state_h, state_c = dec_lstm(dec_embed,\n                                             initial_state=decoder_states_inputs)\n\ndecoder_states = [state_h, state_c]\n\ndec_model = Model([dec_inp] + decoder_states_inputs,\n                  [decoder_outputs] + decoder_states)\n\nfrom keras.preprocessing.sequence import pad_sequences\n\nprint(\"##########################################\")\nprint(\"#              Project Miika             #\")\nprint(\"##########################################\")\n\nprepro1 = \"\"\nwhile prepro1 != 'q':\n    prepro1 = input(\"You : \")\n    ## prepro1 = \"Hello\"\n\n    prepro1 = formatText(prepro1)\n    ## prepro1 = \"hello\"\n\n    prepro = [prepro1]\n    ## prepro1 = [\"hello\"]\n\n    txt = []\n    for x in prepro:\n        # x = \"hello\"\n        tempList = []\n        for y in x.split():\n            ## y = \"hello\"\n            try:\n                tempList.append(vocab[y])\n                ## vocab['hello'] = 454\n            except:\n                tempList.append(vocab['<OUT>'])\n        txt.append(tempList)\n\n    ## txt = [[454]]\n    txt = pad_sequences(txt, 13, padding='post')\n\n    ## txt = [[454,0,0,0,.........13]]\n\n    stat = enc_model.predict(txt)\n\n    empty_target_seq = np.zeros((1, 1))\n    ##   empty_target_seq = [0]\n\n    empty_target_seq[0, 0] = vocab['<SOS>']\n    ##    empty_target_seq = [255]\n\n    stop_condition = False\n    decoded_translation = ''\n\n    while not stop_condition:\n\n        dec_outputs, h, c = dec_model.predict([empty_target_seq] + stat)\n        decoder_concat_input = dense(dec_outputs)\n        ## decoder_concat_input = [0.1, 0.2, .4, .0, ...............]\n\n        sampled_word_index = np.argmax(decoder_concat_input[0, -1, :])\n        ## sampled_word_index = [2]\n\n        sampled_word = inv_vocab[sampled_word_index] + ' '\n\n        ## inv_vocab[2] = 'hi'\n        ## sampled_word = 'hi '\n\n        if sampled_word != '<EOS> ':\n            decoded_translation += sampled_word\n\n        if sampled_word == '<EOS> ' or len(decoded_translation.split()) > 13:\n            stop_condition = True\n\n        empty_target_seq = np.zeros((1, 1))\n        empty_target_seq[0, 0] = sampled_word_index\n        ## <SOS> - > hi\n        ## hi --> <EOS>\n        stat = [h, c]\n\n    print(\"Miika : \", decoded_translation)\n    print(\"==============================================\")\n","metadata":{},"execution_count":null,"outputs":[]}]}